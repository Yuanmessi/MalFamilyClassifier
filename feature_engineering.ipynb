{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8372469-75d5-4c16-951f-1dc9d3dbdd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc8de18-08fb-4018-bc43-f14e15955f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "%run features.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6177192d-4571-4386-91c4-01054079cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693274bb-2ea4-4e61-9d1c-fb7d431d9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ember(obj, sample_path, inter_path):\n",
    "    \"\"\" Feature engineering for ember features. \"\"\"\n",
    "    dirs = sample_path.split('/')\n",
    "    # Data belongs to 'train' or 'test' and file belongs to 'pe' or 'asm'\n",
    "    data_type, file_type = dirs[-2], dirs[-1]\n",
    "    with open(f\"{inter_path}/{data_type}_filename.txt\", 'r') as fp:\n",
    "        filename = fp.read().split()\n",
    "    arr = np.zeros((len(filename), obj.dim))\n",
    "\n",
    "    if file_type == 'pe':\n",
    "        with tqdm(total=len(filename), ncols=80, desc=f\"{data_type}_{obj.name}\") as pbar:\n",
    "            for i, sample in enumerate(filename):\n",
    "                with open(f\"{sample_path}/{sample}\", \"rb\") as f:\n",
    "                    bytez = f.read()\n",
    "                arr[i, :] = obj.feature_vector(bytez)\n",
    "                pbar.update(1)\n",
    "    else:  # file_type == 'asm'\n",
    "        with tqdm(total=len(filename), ncols=80, desc=f\"{data_type}_{obj.name}\") as pbar:\n",
    "            for i, sample in enumerate(filename):\n",
    "                with open(f\"{sample_path}/{sample}.asm\", \"rb\") as f:\n",
    "                    stringz = f.read().decode('utf-8', errors='ignore')\n",
    "                arr[i, :] = obj.feature_vector(stringz)\n",
    "                pbar.update(1)\n",
    "\n",
    "    np.save(f\"{inter_path}/feature/{data_type}_{obj.name}.npy\", arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad1b7d8-3353-46ec-b03b-c353fb7742af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_tfidf_df(obj, sample_path, inter_path):\n",
    "    \"\"\" Save the words of all samples to a DataFrame for tf-idf input. \"\"\"\n",
    "    dirs = sample_path.split('/')\n",
    "    data_type, file_type = dirs[-2], dirs[-1]\n",
    "    with open(f\"{inter_path}/{data_type}_filename.txt\", 'r') as fp:\n",
    "        filename = fp.read().split()\n",
    "    if file_type == 'asm':\n",
    "        filename = [f + '.asm' for f in filename]\n",
    "    all_word_feature = []\n",
    "    with tqdm(total=len(filename), ncols=80, desc=f\"{obj.name_tfidf}_{data_type}\") as pbar:\n",
    "        for sample in filename:\n",
    "            with open(f\"{sample_path}/{sample}\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "                all_word_feature.append(obj.tfidf_features(f))\n",
    "            pbar.update(1)\n",
    "\n",
    "    word_feature = pd.DataFrame({'filename': filename, \"word_feature\": all_word_feature})\n",
    "    word_feature.to_csv(f\"{inter_path}/feature/{data_type}_{obj.name_tfidf}_tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "526d4e63-60a5-4dd2-8d7f-27369aff813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tfidf(obj, inter_path, tfidf_params):\n",
    "    \"\"\" Save the tf-idf model. \"\"\"\n",
    "    \n",
    "    train_words_ = pd.read_csv(f\"{inter_path}/feature/train_{obj.name_tfidf}_tfidf.csv\")\n",
    "    test_words_ = pd.read_csv(f\"{inter_path}/feature/test_{obj.name_tfidf}_tfidf.csv\")\n",
    "    all_words_ = train_words_.append(test_words_)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(**tfidf_params)\n",
    "    vectorizer.fit(all_words_.word_feature.fillna(' ').tolist())\n",
    "\n",
    "    joblib.dump(vectorizer,\n",
    "                open(f\"{inter_path}/models/TFIDF_model_{obj.name_tfidf}_{tfidf_params['max_features']}.pth\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924e38a5-f2ca-4518-9dea-3cdb6fb60381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_tfidf_np(data_type, name_tfidf, inter_path, max_features):\n",
    "    \"\"\" Save the tf-idf feature into numpy format. \"\"\"\n",
    "    vectorizer = joblib.load(open(f\"{inter_path}/models/TFIDF_model_{name_tfidf}_{max_features}.pth\", \"rb\"))\n",
    "    vectorizer.max_features = max_features\n",
    "    words_ = pd.read_csv(f\"{inter_path}/feature/{data_type}_{name_tfidf}_tfidf.csv\")\n",
    "    words = vectorizer.transform(words_.word_feature.fillna(' ').tolist())\n",
    "    np.save(f\"{inter_path}/feature/{data_type}_{name_tfidf}_{max_features}.npy\", words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c10ac2f-21e4-4c56-9f9f-c2396e5480e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_asm2txt(sample_path, inter_path):\n",
    "    \"\"\" Save the opcode of all samples to a txt for asm2vec input. \"\"\"\n",
    "    dirs = sample_path.split('/')\n",
    "    data_type, file_type = dirs[-2], dirs[-1]\n",
    "\n",
    "    def asm2txt_by_datatype(spath, dtype):\n",
    "        with open(f\"{inter_path}/{dtype}_filename.txt\", 'r') as fp:\n",
    "            filenames = fp.read().split()\n",
    "\n",
    "#         if dtype == 'train':\n",
    "#             # Deal with overlap samples in training and test set\n",
    "#             delect789 = list(np.load(f\"{inter_path}/train_filename_de.npy\"))\n",
    "#             filenames = list(set(filenames) - set(delect789))\n",
    "\n",
    "#         elif dtype == 'test':\n",
    "        if dtype == 'test':\n",
    "            spath = spath.replace('train', 'test')\n",
    "\n",
    "        with tqdm(total=len(filenames), ncols=80, desc=f\"{dtype}_asm2txt\") as pbar:\n",
    "            for filename in filenames:\n",
    "                with open(os.path.join(spath, filename) + '.asm', \"r\", encoding='utf-8', errors='ignore') as fp:\n",
    "                    opline_list = OpcodeInfo().asm_to_txt(fp)\n",
    "                with open(os.path.join(f\"{inter_path}/semantic/\", filename) + '.txt', 'w+', encoding='utf-8') as f:\n",
    "                    for line in opline_list:\n",
    "                        f.write(line + '\\n')\n",
    "                pbar.update(1)\n",
    "\n",
    "    asm2txt_by_datatype(sample_path, data_type)\n",
    "    asm2txt_by_datatype(sample_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897d4443-3ac9-4927-ac24-0fe9478352cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_asm2vec(data_type, inter_path):\n",
    "    \"\"\"Feature engineering for asm2vec feature.\"\"\"\n",
    "\n",
    "    if data_type == \"train\":\n",
    "        # Train a Word2vec model by mixing training set and test set\n",
    "        sentences = PathLineSentences(f\"{inter_path}/semantic/\")\n",
    "        model = Word2Vec(sentences=sentences, vector_size=1024, window=5, min_count=5, workers=5)\n",
    "        model.wv.save_word2vec_format(f\"{inter_path}/models/asm2vec.bin\", binary=True, sort_attr='count')\n",
    "\n",
    "    # Load the trained Word2vec model\n",
    "    model_wv = KeyedVectors.load_word2vec_format(f\"{inter_path}/models/asm2vec.bin\", binary=True)\n",
    "\n",
    "    with open(f\"{inter_path}/{data_type}_filename.txt\", 'r') as fp:\n",
    "        filename = fp.read().split()\n",
    "    # Feature engineering for generating string vector features\n",
    "    obj = StringVector()\n",
    "    arr = np.zeros((len(filename), obj.dim))\n",
    "    with tqdm(total=len(filename), ncols=80, desc=obj.name) as pbar:\n",
    "        for i, file in enumerate(filename):\n",
    "            with open(f\"{inter_path}/semantic/{file}.txt\", \"rb\") as f:\n",
    "                stringz = f.read().decode('utf-8', errors='ignore')\n",
    "            lines = ' '.join(stringz.split('\\n'))\n",
    "            raw_words = list(set(lines.split()))\n",
    "            arr[i, :] = obj.feature_vector((model_wv, raw_words))\n",
    "            pbar.update(1)\n",
    "    arr[np.isnan(arr)] = 0\n",
    "    np.save(f\"{inter_path}/feature/{data_type}_semantic.npy\", arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de43e791-788b-4f6f-b588-0b039b645dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fusion(data_type, fused_label, features, inter_path):\n",
    "    arr = [np.load(f\"{inter_path}/feature/{data_type}_{f}.npy\") for f in features]\n",
    "    np.save(f\"{inter_path}/feature/{data_type}_{fused_label}.npy\", np.hstack(arr).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1338c3b8-8a9a-4538-a8b8-0e692c9f0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data_type, data_path, inter_path):\n",
    "    \"\"\" Feature engineering code. \"\"\"\n",
    "\n",
    "    pe_path = f\"{data_path}/{data_type}/pe\"\n",
    "    asm_path = f\"{data_path}/{data_type}/asm\"\n",
    "    \n",
    "    # Generate byte features\n",
    "    pe_objs = [ByteHistogram(), ByteEntropyHistogram(), StringExtractor()]\n",
    "    for obj in pe_objs:\n",
    "        feature_ember(obj, pe_path, inter_path)\n",
    "\n",
    "    # Generate format features\n",
    "    asm_objs = [SectionInfo(), ImportsInfo(), ExportsInfo()]\n",
    "    for obj in asm_objs:\n",
    "        feature_ember(obj, asm_path, inter_path)\n",
    "\n",
    "\n",
    "    if data_type == 'train':\n",
    "        # Generate statistical features\n",
    "        feature_tfidf_df(StringExtractor(), pe_path, inter_path)\n",
    "        feature_tfidf_df(StringExtractor(), pe_path.replace('train', 'test'), inter_path)\n",
    "\n",
    "        feature_tfidf_df(OpcodeInfo(), asm_path, inter_path)\n",
    "        feature_tfidf_df(OpcodeInfo(), asm_path.replace('train', 'test'), inter_path)\n",
    "\n",
    "        # Generate TF-IDF models\n",
    "        words_tf_params1 = {'max_features': 1000}\n",
    "        model_tfidf(StringExtractor(), inter_path, words_tf_params1)\n",
    "        words_tf_params2 = {'max_features': 300}\n",
    "        model_tfidf(StringExtractor(), inter_path, words_tf_params2)\n",
    "        ins_tf_params = {'ngram_range': (1, 3), 'max_features': 1000}\n",
    "        model_tfidf(OpcodeInfo(), inter_path, ins_tf_params)\n",
    "\n",
    "        feature_asm2txt(asm_path, inter_path)\n",
    "\n",
    "    # Generate TF-IDF feature\n",
    "    feature_tfidf_np(data_type, 'words', inter_path, max_features=300)\n",
    "    feature_tfidf_np(data_type, 'words', inter_path, max_features=1000)\n",
    "    feature_tfidf_np(data_type, 'ins', inter_path, max_features=1000)\n",
    "\n",
    "    # Generate asm2vec feature\n",
    "    feature_asm2vec(data_type, inter_path)\n",
    "\n",
    "    # Feature fusion\n",
    "    feature_fusion(data_type, 'ember', ['histogram', 'byteentropy', 'strings'], inter_path)\n",
    "    feature_fusion(data_type, 'ember_section_ins_words', ['ember', 'section', 'ins_1000', 'words_300'], inter_path)\n",
    "    feature_fusion(data_type, 'ember_section_ins_semantic', ['ember', 'section', 'ins_1000', 'semantic'], inter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7985c9-3c53-4c4d-9c54-f3ea308ef9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
