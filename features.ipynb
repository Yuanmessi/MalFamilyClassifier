{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a304a9-d9fa-4b40-8303-54084e18a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3e25e8-8e41-43a2-9b4b-02f48ad62913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureType(object):\n",
    "    \"\"\" Base class from which each feature type may inherit. \"\"\"\n",
    "\n",
    "    name = ''\n",
    "    dim = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, self.dim)\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        \"\"\" Generate a JSON-able representation of the file. \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        \"\"\" Generate a feature vector from the raw features. \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def feature_vector(self, bytez):\n",
    "        \"\"\" Directly calculate the feature vector from the sample itself. This should only be implemented differently\n",
    "        if there are significant speedups to be gained from combining the two functions. \"\"\"\n",
    "        return self.process_raw_features(self.raw_features(bytez))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd38b09d-a269-480b-a253-e7ea099e7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteHistogram(FeatureType):\n",
    "    \"\"\" Byte histogram (count + non-normalized) over the entire binary file. \"\"\"\n",
    "\n",
    "    name = 'histogram'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)\n",
    "        return counts.tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5983d9d1-520a-414b-8267-96adfcbd09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteEntropyHistogram(FeatureType):\n",
    "    \"\"\" 2d byte/entropy histogram, which roughly approximates the joint probability of byte value and local entropy. \"\"\"\n",
    "\n",
    "    name = 'byteentropy'\n",
    "    dim = 256\n",
    "\n",
    "    def __init__(self, step=1024, window=2048):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self.step = step\n",
    "        self.window = window\n",
    "\n",
    "    def _entropy_bin_counts(self, block):\n",
    "        # Coarse histogram, 16 bytes per bin\n",
    "        # 16-bin histogram\n",
    "        c = np.bincount(block >> 4, minlength=16)\n",
    "        p = c.astype(np.float32) / self.window\n",
    "        # Filter non-zero elements\n",
    "        wh = np.where(c)[0]\n",
    "        # \"* 2\" b.c. we reduced information by half: 256 bins (8 bits) to 16 bins (4bits)\n",
    "        H = np.sum(-p[wh] * np.log2(p[wh])) * 2\n",
    "        # Up to 16 bins (max entropy is 8 bits)\n",
    "        Hbin = int(H * 2)\n",
    "        # Handle entropy = 8.0 bits\n",
    "        if Hbin == 16:\n",
    "            Hbin = 15\n",
    "        return Hbin, c\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        output = np.zeros((16, 16), dtype=np.int32)\n",
    "        a = np.frombuffer(bytez, dtype=np.uint8)\n",
    "        if a.shape[0] < self.window:\n",
    "            Hbin, c = self._entropy_bin_counts(a)\n",
    "            output[Hbin, :] += c\n",
    "        else:\n",
    "            # Strided trick\n",
    "            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)\n",
    "            strides = a.strides + (a.strides[-1],)\n",
    "            blocks = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::self.step, :]\n",
    "            # From the blocks, compute histogram\n",
    "            for block in blocks:\n",
    "                Hbin, c = self._entropy_bin_counts(block)\n",
    "                output[Hbin, :] += c\n",
    "        return output.flatten().tolist()\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        counts = np.array(raw_obj, dtype=np.float32)\n",
    "        sum = counts.sum()\n",
    "        normalized = counts / sum\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d8810c-fe49-4b78-bfc5-7ee2d336b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringExtractor(FeatureType):\n",
    "    \"\"\" Extracts strings from raw byte stream of PE or asm file. \"\"\"\n",
    "\n",
    "    name = 'strings'\n",
    "    name_tfidf = 'words'\n",
    "    dim = 1 + 1 + 1 + 96 + 1 + 1 + 1 + 1 + 1  # 104\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        # All consecutive runs of printable string that are 5+ characters\n",
    "        self._allstrings = re.compile(b'[\\x20-\\x7f]{5,}')\n",
    "        # Occurrences of the string 'C:\\', not actually extracting the path.\n",
    "        self._paths = re.compile(b'c:\\\\\\\\', re.IGNORECASE)\n",
    "        # Occurrences of 'http://' or 'https://', not actually extracting the URLs.\n",
    "        self._urls = re.compile(b'https?://', re.IGNORECASE)\n",
    "        # Occurrences of the string prefix 'HKEY_', not actually extracting registry names.\n",
    "        self._registry = re.compile(b'HKEY_')\n",
    "        # Crude evidence of an MZ header (PE dropper or bubbled executable) somewhere in the byte stream\n",
    "        self._mz = re.compile(b'MZ')\n",
    "        # all words which can read\n",
    "        self._words = re.compile(b\"[a-zA-Z]+\")\n",
    "\n",
    "    def tfidf_features(self, bytez):\n",
    "        \"\"\" Extracts a sequence of readable strings from the pe file as original word feature.\"\"\"\n",
    "        words = []\n",
    "        raw_words = re.findall('[a-zA-Z]+', bytez.read())\n",
    "        words_space = ' '.join(w for w in raw_words if 4 < len(w) < 20)\n",
    "        vowel = 'aeiou'\n",
    "        meaningless = ['abcdef', 'aaaaaa']\n",
    "        for text in words_space.split():\n",
    "            if any([w in text.lower() for w in vowel]) and all([w not in text.lower() for w in meaningless]):\n",
    "                words.append(text)\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        allstrings = self._allstrings.findall(bytez)\n",
    "        if allstrings:\n",
    "            # Statistics about strings\n",
    "            string_lengths = [len(s) for s in allstrings]\n",
    "            avlength = sum(string_lengths) / len(string_lengths)\n",
    "            # Map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive\n",
    "            as_shifted_string = [b - ord(b'\\x20') for b in b''.join(allstrings)]\n",
    "            # Histogram count\n",
    "            c = np.bincount(as_shifted_string, minlength=96)\n",
    "            # Distribution of characters in printable strings (entropy)\n",
    "            csum = c.sum()\n",
    "            p = c.astype(np.float32) / csum\n",
    "            wh = np.where(c)[0]\n",
    "            H = np.sum(-p[wh] * np.log2(p[wh]))\n",
    "        else:\n",
    "            avlength = 0\n",
    "            c = np.zeros((96,), dtype=np.float32)\n",
    "            csum = 0\n",
    "            H = 0\n",
    "        return {\n",
    "            'numstrings': len(allstrings),\n",
    "            'avlength': avlength,\n",
    "            'printabledist': c.tolist(),\n",
    "            'printables': int(csum),\n",
    "            'entropy': float(H),\n",
    "            'paths': len(self._paths.findall(bytez)),\n",
    "            'urls': len(self._urls.findall(bytez)),\n",
    "            'registry': len(self._registry.findall(bytez)),\n",
    "            'MZ': len(self._mz.findall(bytez))\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        hist_divisor = float(raw_obj['printables']) if raw_obj['printables'] > 0 else 1.0\n",
    "        return np.hstack([\n",
    "            raw_obj['numstrings'], raw_obj['avlength'], raw_obj['printables'],\n",
    "            np.asarray(raw_obj['printabledist']) / hist_divisor, raw_obj['entropy'], raw_obj['paths'], raw_obj['urls'],\n",
    "            raw_obj['registry'], raw_obj['MZ']\n",
    "        ]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200beab7-1f34-4160-b8a5-b7aa6c356e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionInfo(FeatureType):\n",
    "    \"\"\" Information about section names, sizes and certain special sections. Uses hashing trick to summarize all this\n",
    "    section info into a feature vector. \"\"\"\n",
    "\n",
    "    name = 'section'\n",
    "    dim = 10 + 50 + 50 + 50 + 50  # 210\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        # Beginning of the section\n",
    "        self._section = re.compile(r'; Section (\\d+)')\n",
    "        # Segment names\n",
    "        self._name = re.compile(r'\\n(\\S+)\\s+segment')\n",
    "        self._idata = re.compile(r'; (_idata)')\n",
    "        # Section size in file and virtual size\n",
    "        self._size = re.compile(r'Section size\\sin file.*?[(]\\s*?(\\d+)')\n",
    "        self._vsize = re.compile(r'Virtual size.*?[(]\\s*?(\\d+)')\n",
    "        # Characteristics of section\n",
    "        self._properties = re.compile(r'Flags\\s+\\w+:\\s(.+)')\n",
    "        self._entryperm = re.compile(r'Segment permissions:\\s(\\S+)')\n",
    "        self._entrytype = re.compile(r'Segment\\stype:\\s+(.+)')\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        # Collect infos by section order\n",
    "        section_size = self._size.findall(bytez)\n",
    "        section_size = [int(s) for s in section_size]\n",
    "        virtual_size = self._vsize.findall(bytez)\n",
    "        virtual_size = [int(v) for v in virtual_size]\n",
    "        properties = [p.split() for p in self._properties.findall(bytez)]\n",
    "        # Restrict the scope of sections\n",
    "        section_id = [int(s) for s in self._section.findall(bytez)]\n",
    "        section_pos = [pos.span()[0] for pos in self._section.finditer(bytez)]\n",
    "        # Infos of first executable section are complete (but need to be discarded)\n",
    "        if 1 in section_id:\n",
    "            section_pos.pop(0)\n",
    "            section_size.pop(0)\n",
    "            virtual_size.pop(0)\n",
    "            properties.pop(0)\n",
    "        section_pos.append(len(bytez))\n",
    "        section_name = []\n",
    "        for i in range(1, len(section_pos)):\n",
    "            # Integrate segments into sections\n",
    "            segment_name = self._name.findall(bytez, section_pos[i - 1], section_pos[i])\n",
    "            is_idata = self._idata.findall(bytez, section_pos[i - 1], section_pos[i])\n",
    "            if len(is_idata) != 0:\n",
    "                segment_name.extend(is_idata)\n",
    "            section_name.append(' '.join(segment_name))\n",
    "        # Entry point, that is, the first executable section\n",
    "        entry_sec = self._name.findall(bytez, 0, section_pos[0])\n",
    "        is_idata = self._idata.findall(bytez, 0, section_pos[0])\n",
    "        entry_props = []\n",
    "        if len(entry_sec) == 0:\n",
    "            if len(is_idata) != 0:\n",
    "                entry_section = is_idata[0]\n",
    "            else:\n",
    "                return {} # No section infos\n",
    "        else:\n",
    "            entry_section = entry_sec[0]\n",
    "            entry_type = self._entrytype.findall(bytez, 0, section_pos[0])\n",
    "            entry_perm = self._entryperm.findall(bytez, 0, section_pos[0])\n",
    "            entry_types = []\n",
    "            for et in entry_type:\n",
    "                if et != 'Externs':\n",
    "                    et_str = et.split()[-1].capitalize().replace('Code', 'Text')\n",
    "                    entry_types.extend(et_str)\n",
    "            perms = []\n",
    "            for ep in entry_perm:\n",
    "                ep_str = ep.replace('Read', 'Readable').replace('Write', 'Writable').replace('Execute', 'Executable')\n",
    "                perms.extend(ep_str.split('/'))\n",
    "            entry_perms = list(set(perms))\n",
    "            entry_perms.sort(key=perms.index)\n",
    "            entry_props = entry_types + entry_perms\n",
    "        raw_obj = {\"entry\": {'name': entry_section, 'props': entry_props},\n",
    "                   \"sections\": [{'name': name, 'size': size, 'vsize': vsize, 'props': props}\n",
    "                                for name, size, vsize, props in zip(section_name, section_size, virtual_size, properties)]}\n",
    "        return raw_obj\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        if not raw_obj:\n",
    "            return np.zeros((self.dim, ), dtype= np.float32)\n",
    "        entry = raw_obj['entry']\n",
    "        sections = raw_obj['sections']\n",
    "        # Split the section name into segment names\n",
    "        segments = [entry['name']]\n",
    "        segments.extend(s['name'] for s in sections)\n",
    "        segments = ' '.join(segments).split()\n",
    "        # Get the permission of RX/W of first executable section\n",
    "        rx = 1 if 'Readable' in entry['props'] and 'Executable' in entry['props'] else 0\n",
    "        w = 1 if 'Writable' in entry['props'] else 0\n",
    "        general = [\n",
    "            # Total number of sections\n",
    "            len(sections) + 1,\n",
    "            # Total number of segments\n",
    "            len(segments),\n",
    "            # number of sections with zero size\n",
    "            sum(1 for s in sections if s['size'] == 0),\n",
    "            # number of sections with an empty name\n",
    "            sum(1 for s in sections if s['name'] == ''),\n",
    "            # Number of RX\n",
    "            sum(1 for s in sections if 'Readable' in s['props'] and 'Executable' in s['props']) + rx,\n",
    "            # Number of W\n",
    "            sum(1 for s in sections if 'Writable' in s['props']) + w,\n",
    "            # If debug section exists\n",
    "            1 if '_debug' in segments else 0,\n",
    "            # If relocation section exists\n",
    "            1 if '_reloc' in segments else 0,\n",
    "            # If resource section exists\n",
    "            1 if '_rsrc' in segments else 0,\n",
    "            # If thread local storage section exists\n",
    "            1 if '_tls' in segments else 0\n",
    "        ]\n",
    "        # Gross characteristics of each section\n",
    "        section_sizes = [(s['name'], s['size']) for s in sections]\n",
    "        section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n",
    "        section_vsize = [(s['name'], s['vsize']) for s in sections]\n",
    "        section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n",
    "        entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([entry['name']]).toarray()[0]\n",
    "        characteristics = entry['props'] + [p for s in sections for p in s['props'] if s['name'].startswith(entry['name'])]\n",
    "        characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n",
    "        return np.hstack([\n",
    "            general, section_sizes_hashed, section_vsize_hashed, entry_name_hashed, characteristics_hashed\n",
    "        ]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4b7249-c97d-4a5b-80fc-c14fb7247e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportsInfo(FeatureType):\n",
    "    \"\"\" Information about imported libraries and functions from the import address table. \"\"\"\n",
    "\n",
    "    name = 'imports'\n",
    "    dim = 128 + 512  # 640\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        # Imported libraries and functions\n",
    "        self._libraries = re.compile(r'Imports from (.*?dll)')\n",
    "        self._functions = re.compile(r'extrn (\\w+)')\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        imports = {}\n",
    "        # Restrict the function area of libraries\n",
    "        libraries = self._libraries.findall(bytez)\n",
    "        if len(libraries) == 0:\n",
    "            return imports\n",
    "        library_pos = [pos.span()[0] for pos in self._libraries.finditer(bytez)]\n",
    "        library_pos.append(len(bytez))\n",
    "        for i in range(1, len(library_pos)):\n",
    "            functions = self._functions.findall(bytez, library_pos[i - 1], library_pos[i])\n",
    "            functions = [f.replace('__imp_', '') for f in functions]\n",
    "            # Libraries can be duplicated in listing, extend instead of overwrite\n",
    "            if libraries[i - 1] not in imports:\n",
    "                imports[libraries[i - 1]] = functions\n",
    "            else:\n",
    "                imports[libraries[i - 1]].extend(functions)\n",
    "        return imports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        # Unique libraries\n",
    "        libraries = list(set([l.lower() for l in raw_obj.keys()]))\n",
    "        libraries_hashed = FeatureHasher(128, input_type=\"string\").transform([libraries]).toarray()[0]\n",
    "        # A string like \"kernel32.dll:CreateFileMappingA\" for each imported function\n",
    "        imports = [lib.lower() + ':' + e for lib, elist in raw_obj.items() for e in elist]\n",
    "        imports_hashed = FeatureHasher(512, input_type=\"string\").transform([imports]).toarray()[0]\n",
    "        return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b35733-5775-4f0b-81d2-14ba6bdc6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportsInfo(FeatureType):\n",
    "    \"\"\" Information about exported functions. \"\"\"\n",
    "\n",
    "    name = 'exports'\n",
    "    dim = 128\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        # Exported functions\n",
    "        self._functions = re.compile(r'Exported entry\\s+\\d.\\s(\\w+)')\n",
    "\n",
    "    def raw_features(self, bytez):\n",
    "        functions = self._functions.findall(bytez)\n",
    "        if len(functions) == 0:\n",
    "            return []\n",
    "        exports = list(set([f.lower() for f in functions]))\n",
    "        return exports\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        exports_hashed = FeatureHasher(128, input_type=\"string\").transform([raw_obj]).toarray()[0]\n",
    "        return exports_hashed.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f751360e-44a4-48d4-acf1-a467cb227512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpcodeInfo(FeatureType):\n",
    "    \"\"\" Information about 'interesting' Opcode N-gram from the asm file.\"\"\"\n",
    "\n",
    "    name_tfidf = 'ins'\n",
    "\n",
    "    # dim = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self.start_key_ = 'Pure code'\n",
    "        self.end_key_ = \"ends\"\n",
    "        self.run_ = 0\n",
    "        self.evasion_key_ = ['dd', 'dq', 'db', 'dw', 'unicode', ';org', 'assume', 'align', ';', 'public']\n",
    "        # Data/Pointer/Index Register\n",
    "        self.register_ = [\n",
    "            'eax', 'ebx', 'ecx', 'edx', 'esi', 'edi', 'ebp', 'esp',\n",
    "            'ah', 'al', 'bh', 'bl', 'ch', 'cl', 'dh', 'dl',\n",
    "            'ax', 'bx', 'cx', 'dx', 'si', 'di', 'bp', 'sp'\n",
    "        ]\n",
    "        self.no_call_ = ['sub_', 'loc_', 'dword_', 'unknown_']\n",
    "\n",
    "    def tfidf_features(self, bytez):\n",
    "        \"\"\" Extracts Opcode-like sequence from the asm file as original word feature.\"\"\"\n",
    "        ins_list = []\n",
    "        run = 0\n",
    "        for line in bytez:\n",
    "            if self.start_key_ in line:\n",
    "                run = 1\n",
    "            elif self.end_key_ in line:\n",
    "                run = 0\n",
    "            elif run == 1:\n",
    "                line_xx = line.split()\n",
    "                if len(line_xx) > 1 and line_xx[0].isalpha() and line_xx[0] not in self.evasion_key_:\n",
    "                    op = line_xx[0]\n",
    "                    # Add function call\n",
    "                    if line_xx[0] == 'call' and all([not line_xx[1].startswith(nc) for nc in self.no_call_]):\n",
    "                        op += line_xx[1]\n",
    "                    # Add registers\n",
    "                    else:\n",
    "                        line_reg = line_xx[1].split(',')[0]\n",
    "                        if line_reg in self.register_:\n",
    "                            op += line_reg\n",
    "                    if line_xx[-2] == ';':\n",
    "                        op += line_xx[-1]\n",
    "                    ins_list.append(op)\n",
    "        return ins_list\n",
    "\n",
    "    def asm_to_txt(self, bytez):\n",
    "        \"\"\" Asm opcode & string save to the txt file. \"\"\"\n",
    "        opline = ''\n",
    "        opline_list = []\n",
    "        run = 0\n",
    "        for line in bytez:\n",
    "            if self.start_key_ in line:\n",
    "                run = 1\n",
    "            elif self.end_key_ in line:\n",
    "                run = 0\n",
    "            elif run == 1:\n",
    "                line_xx = line.split()\n",
    "                if len(line_xx) > 1 and line_xx[0].isalpha() and line_xx[0] not in self.evasion_key_:\n",
    "                    opline += line_xx[0] + ' '\n",
    "                    # Add function call\n",
    "                    if line_xx[0] == 'call' and all([not line_xx[1].startswith(nc) for nc in self.no_call_]):\n",
    "                        opline += line_xx[1] + ' '\n",
    "                    # Add registers\n",
    "                    else:\n",
    "                        line_reg = line_xx[1].split(',')[0]\n",
    "                        if line_reg in self.register_:\n",
    "                            opline += line_reg + ' '\n",
    "                    if line_xx[-2] == ';':\n",
    "                        opline += line_xx[-1] + ' '\n",
    "                elif len(opline) > 0:\n",
    "                    opline_list.append(opline)\n",
    "                    opline = ''\n",
    "        return opline_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bffb7b65-5986-4d76-8e62-a293a44d5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringVector(FeatureType):\n",
    "    \"\"\" String vector generated from .asm file. \"\"\"\n",
    "\n",
    "    name = 'semantic'\n",
    "    dim = 1024\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, model_words):\n",
    "        model_wv = model_words[0]\n",
    "        raw_words = model_words[1]\n",
    "        vector_list = [model_wv[key] for key in raw_words if key in model_wv]\n",
    "        vector_arr = np.array(vector_list)\n",
    "        return vector_arr\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.mean(raw_obj, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e1c53-6e71-446a-8847-fda5065f26c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
